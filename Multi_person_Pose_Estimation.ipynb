{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kacychou/multi-person-pose-estimation/blob/main/Multi_person_Pose_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42_MUVu9tLwo"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.7.0\n",
        "!pip install tensorflow-gpu==2.7.0\n",
        "!pip install tensorflow-hub opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fyFo72yuUWM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBbeBzPZuiFF"
      },
      "outputs": [],
      "source": [
        "#turn on memory growth to allocate as much GPU memory as needed for runtime alloctions - avoid out of memory error\n",
        "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "  tf.config.experimental.set_memory_growth(gpu,True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIrf-X09v7hz"
      },
      "source": [
        "1. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQLYNR1Auw7J"
      },
      "outputs": [],
      "source": [
        "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1') #download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldaSui7cvyfb"
      },
      "outputs": [],
      "source": [
        "movenet=model.signatures['serving_default'] #extract model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMwUXqf0wZYg"
      },
      "source": [
        "2. Make Detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFrC3QmiJnn7"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting list of \n",
        "data_files = glob.glob(\"/content/drive/My Drive/Colab Notebooks/*.mp4\")"
      ],
      "metadata": {
        "id": "gCLZpGG2zqoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcobbdCewqC9"
      },
      "outputs": [],
      "source": [
        "#keypoints_with_scores -> set of keypoints for 6 different ppl\n",
        "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
        "  for person in keypoints_with_scores:\n",
        "    draw_connections(frame,person,edges,confidence_threshold) #our rendering functions #render to our image\n",
        "    draw_keypoints(frame,person,confidence_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-LsliCCkotb"
      },
      "outputs": [],
      "source": [
        "#actual image, frame, keypoints of a single person, confindence threshold e.g. dont draw anything that is below 0.25 \n",
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    \n",
        "    for kp in shaped:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8iEaQnUYLlK"
      },
      "outputs": [],
      "source": [
        "EDGES = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejljc73RYZRs"
      },
      "outputs": [],
      "source": [
        "#edges -> tell us what joints connect to what other joints\n",
        "#e.g. nose connects to left eye 0,1\n",
        "#nose connects to right eye 0,2\n",
        "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    \n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = shaped[p1]\n",
        "        y2, x2, c2 = shaped[p2]\n",
        "        \n",
        "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
        "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y9dKJQF3d_C"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(data_files[0]) #reads video from the file path\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "   \n",
        "size = (frame_width, frame_height)\n",
        "result = cv2.VideoWriter('/content/drive/My Drive/Colab Notebooks/final1.avi', \n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         20, size)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    #Resize image\n",
        "    if frame is None:\n",
        "      print(\"no image passed\")\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      img = frame.copy()\n",
        "      img = tf.image.resize_with_pad(tf.expand_dims(img,axis=0),256,256) #automatically pad our image to 0 if we reshape to odd size\n",
        "      input_img = tf.cast(img,dtype=tf.int32) #convert type to a 32-bit integer\n",
        "  \n",
        "\n",
        "    # Detection section\n",
        "      results = movenet(input_img)\n",
        "      keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6,17,3)) #apply transformation so we only have keypoints with score #6 ppl/17 kp/3 value for each kp\n",
        "    #now we have a single array for every kp\n",
        "      print(keypoints_with_scores) #values used for rendering #returns [y co-ord, x co-ord, score (detection confidence)]\n",
        "    # Render keypoints e.g. 17 keypoints -> nose, left eye\n",
        "    #loop_through_people(frame, keypoints_with_scores, EDGES, 0.1)\n",
        "      loop_through_people(frame, keypoints_with_scores,EDGES,0.3)\n",
        "      result.write(frame)\n",
        "      cv2_imshow(frame)\n",
        "      if cv2.waitKey(10) & 0xFF==ord('q'): #how we want to exit\n",
        "          break\n",
        "result.release()\n",
        "cv2.destroyAllWindows() #close windows\n",
        "cap.release() #release our webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCT2wofHl4SY"
      },
      "outputs": [],
      "source": [
        "keypoints_with_scores[0] \n",
        "#keypoints of the first person\n",
        "#unnormalised coordinates -> not scaled the size for the image\n",
        "# 3rd value shows how confidence the move net model is in predicting that particular coordinate\n",
        "#upper body + lower body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxNhbmK2foHE"
      },
      "outputs": [],
      "source": [
        "results\n",
        "#a set of array - wrapped inside a single array\n",
        "#6 people\n",
        "#56 values inside each result\n",
        "#values represent (y,x,score) * 17 key points\n",
        "#remaining 5 -> bounding box values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['output_0'].numpy()[:,:,:51] #convert to numpy array -> all 1, all 6, get me the first 51 values"
      ],
      "metadata": {
        "id": "cMk6Mrk9bVvB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Multi-person Pose Estimation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJzBsKv5Db+18RIH9OOf5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}